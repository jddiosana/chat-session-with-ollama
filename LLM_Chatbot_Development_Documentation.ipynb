{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing the Basic LLM Chatbot with Ollama\n",
    "\n",
    "This notebook documents the development of a basic LLM-powered chatbot using LangChain, Ollama, PostgreSQL, and Streamlit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Choice of Offline LLM API Platform and Model\n",
    "\n",
    "The project was developed in a **rapid prototyping approach**, so we prioritized ease of setup and integration. For this reason, we chose **Ollama** as the offline LLM API platform. The following factors were considered why we chose Ollama for the chabot development:\n",
    "- Easy local setup (tutorial: [https://ollama.com/download/linux](https://ollama.com/download/linux))\n",
    "- Docker support (see: [https://ollama.com/blog/ollama-is-now-available-as-an-official-docker-image](https://ollama.com/blog/ollama-is-now-available-as-an-official-docker-image))\n",
    "- Streamlined model serving with REST endpoints\n",
    "- Compatible with LangChain\n",
    "\n",
    "For the model, we chose **Llama 3.2** mainly for its 1) exceptional performance and efficiency, and 2) due to hardware constraints. Its lightweight nature also allows for more portability when deploying the chatbot on different machines.\n",
    "\n",
    "The model was pulled using the following command:\n",
    "```\n",
    "ollama pull llama3.2\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting Up PostgreSQL with Docker\n",
    "\n",
    "We use Docker Compose to spin up a PostgreSQL instance.\n",
    "\n",
    "**`docker-compose.yml` excerpt:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Do not run this cell! It is only for documentation purposes.\n",
    "\n",
    "services:\n",
    "  db:\n",
    "    image: postgres:14\n",
    "    environment:\n",
    "      POSTGRES_USER: postgres\n",
    "      POSTGRES_PASSWORD: password\n",
    "      POSTGRES_DB: chat-history\n",
    "    ports:\n",
    "      - \"5432:5432\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also start a PostgreSQL instance independently in Docker by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Do not run this cell! It is only for documentation purposes.\n",
    "\n",
    "docker run -d --name postgres-db -e POSTGRES_DB=chat-history -e POSTGRES_USER=postgres -e POSTGRES_PASSWORD=password -p 5432:5432 postgres:14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Backend: LangChain + langchain-postgres\n",
    "\n",
    "The backend is built using LangChain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started the backend by using the `langchain-ollama` to connect to the Ollama model and get a response. A sample code is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm just a language model, so I don't have emotions or feelings like humans do. However, I'm functioning properly and ready to assist you with any questions or tasks you may have! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "# NOTE: The variables in this notebook are based on the active session I have in my local machine.\n",
    "# You may need to change the values if you want to run the code.\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OLLAMA_MODEL = os.getenv(\"OLLAMA_MODEL\", \"llama3.2\")\n",
    "OLLAMA_URL = os.getenv(\"OLLAMA_URL\", \"http://localhost:11434\")\n",
    "\n",
    "llm = ChatOllama(model=OLLAMA_MODEL, base_url=OLLAMA_URL)\n",
    "\n",
    "response = llm.invoke(\"Hello, how are you?\")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we started building the database connection using `langchain-postgres` to store the chat history. The following code snippet shows how we use `PostgresChatMessageHistory` to persist chat sessions in the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For creating tables in the database, we can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The variables in this notebook are based on the active session I have in my local machine.\n",
    "# You may need to change the values if you want to run the code.\n",
    "\n",
    "from langchain_postgres import PostgresChatMessageHistory\n",
    "import psycopg\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "CHAT_HISTORY_TABLE = os.getenv(\"CHAT_HISTORY_TABLE\", \"chat_history\")\n",
    "\n",
    "def get_connection():\n",
    "    return psycopg.connect(\n",
    "        host=os.getenv(\"POSTGRES_HOST\", \"localhost\"),\n",
    "        port=os.getenv(\"POSTGRES_PORT\", 5432),\n",
    "        dbname=os.getenv(\"POSTGRES_DB\", \"chat-history\"),\n",
    "        user=os.getenv(\"POSTGRES_USER\", \"postgres\"),\n",
    "        password=os.getenv(\"POSTGRES_PASSWORD\", \"password\")\n",
    "    )\n",
    "\n",
    "PostgresChatMessageHistory.create_tables(\n",
    "    get_connection(),\n",
    "    CHAT_HISTORY_TABLE # table name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Describe what is an AI chatbot for beginners!', additional_kwargs={}, response_metadata={}),\n",
       " AIMessageChunk(content=\"An AI chatbot is a computer program that uses artificial intelligence (AI) to simulate human-like conversations with users through text or voice interactions. It's designed to answer questions, provide information, and perform tasks using natural language processing (NLP). Chatbots can be simple or complex, but their primary goal is to make interacting with technology feel more human-like and intuitive.\", additional_kwargs={}, response_metadata={'done': True, 'model': 'llama3.2', 'created_at': '2025-06-08T10:51:05.590721898Z', 'eval_count': 75, 'model_name': 'llama3.2', 'done_reason': 'stop', 'eval_duration': 18058476023, 'load_duration': 16522514, 'total_duration': 19635443293, 'prompt_eval_count': 83, 'prompt_eval_duration': 1559748814}, id='run--d988a83f-48ad-4a30-a2b9-ba5b848b6b2a', usage_metadata={'input_tokens': 83, 'output_tokens': 75, 'total_tokens': 158})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: The variables in this notebook are based on the active session I have in my local machine.\n",
    "# You may need to change the values if you want to run the code.\n",
    "\n",
    "from langchain_postgres import PostgresChatMessageHistory\n",
    "import psycopg\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "session_id = \"c2b34f93-1dfa-49a0-9bc3-899c3465314e\" # sample session_id\n",
    "\n",
    "def get_connection():\n",
    "    return psycopg.connect(\n",
    "        host=os.getenv(\"POSTGRES_HOST\", \"localhost\"),\n",
    "        port=os.getenv(\"POSTGRES_PORT\", 5432),\n",
    "        dbname=os.getenv(\"POSTGRES_DB\", \"chat-history\"),\n",
    "        user=os.getenv(\"POSTGRES_USER\", \"postgres\"),\n",
    "        password=os.getenv(\"POSTGRES_PASSWORD\", \"password\")\n",
    "    )\n",
    "\n",
    "chat_history = PostgresChatMessageHistory(\n",
    "    \"chat_history\", # table name\n",
    "    session_id, # session id\n",
    "    sync_connection=get_connection()\n",
    ")\n",
    "\n",
    "chat_history.get_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the chat history is stored in the database. The next step is to be able to use the chat history to give more context to the LLM when generating a response.\n",
    "\n",
    "For this, I chained the `PostgresChatMessageHistory` with the `ChatOllama` model and used the `RunnableWithMessageHistory` to give the chat history to the LLM.\n",
    "\n",
    "See how we were able to give the chat history to the LLM in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The variables in this notebook are based on the active session I have in my local machine.\n",
    "# You may need to change the values if you want to run the code.\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_postgres.chat_message_histories import PostgresChatMessageHistory\n",
    "\n",
    "from app.core.prompts import get_session_title_prompt, chat_prompt\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "CHAT_HISTORY_TABLE = os.getenv(\"CHAT_HISTORY_TABLE\", \"chat_history\")\n",
    "session_id = \"c2b34f93-1dfa-49a0-9bc3-899c3465314e\" # sample session_id\n",
    "\n",
    "llm = ChatOllama(model=OLLAMA_MODEL, base_url=OLLAMA_URL)\n",
    "\n",
    "sync_connection = get_connection()\n",
    "\n",
    "def get_session_title_chain():\n",
    "    return get_session_title_prompt() | llm\n",
    "\n",
    "def get_chat_chain():\n",
    "    return chat_prompt() | llm\n",
    "\n",
    "def get_chat_chain_with_history(chat_history_table, connection):\n",
    "    return RunnableWithMessageHistory(\n",
    "        get_chat_chain(),\n",
    "        lambda session_id: PostgresChatMessageHistory(\n",
    "            chat_history_table,\n",
    "            session_id,\n",
    "            sync_connection=connection\n",
    "        ),\n",
    "        input_messages_key=\"user_input\",\n",
    "        history_messages_key=\"history\"\n",
    "    )\n",
    "\n",
    "# give it a try\n",
    "llm_with_history = get_chat_chain_with_history(CHAT_HISTORY_TABLE, get_connection())\n",
    "\n",
    "response = llm_with_history.invoke({\"user_input\": \"What's my last message?\"}, config={\"configurable\": {\"session_id\": session_id}})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that the LLM was able to give the last message from the chat history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your last message was: \"Describe what is an AI chatbot for beginners!\"\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The runnable `RunnableWithMessageHistory` was used because it allows to achieve the core requirement to store chatbot memory with `langchain-postgres`. It's a wrapper that connects to the memory (Postgres), retrieves the chat history of a given session and passes it to the LLM, and it also automatically updates the memory with the new message and the response from the LLM. It simplifies the process of memory preservation, and, at the same time, it eases the session handling for the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, the following are the steps we took to develop the basic chaining of the LLM:\n",
    "\n",
    "1. We started by creating a simple chain that uses the `ChatOllama` model to generate a response.\n",
    "2. We then added the `PostgresChatMessageHistory` to the chain to store the chat history.\n",
    "3. We then integrated the LLM and the database through the `RunnableWithMessageHistory` wrapper.\n",
    "\n",
    "In the next steps, we will be discussing the additional features that we added to the chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow the user to select from past sessions, we created a feature that allows user to see the previous sessions by listing the distinct session IDs in the database.\n",
    "\n",
    "A sample code is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[UUID('9029ee99-c701-468a-820d-addb23ff1281'),\n",
       " UUID('c2b34f93-1dfa-49a0-9bc3-899c3465314e')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: The variables in this notebook are based on the active session I have in my local machine.\n",
    "# You may need to change the values if you want to run the code.\n",
    "\n",
    "def list_sessions():\n",
    "    cursor = get_connection().cursor()\n",
    "    # get distinct sessions\n",
    "    cursor.execute(\"\"\"\n",
    "        WITH latest_messages AS (\n",
    "            SELECT session_id, MAX(created_at) as last_activity\n",
    "            FROM chat_history\n",
    "            GROUP BY session_id\n",
    "        )\n",
    "        SELECT session_id \n",
    "        FROM latest_messages \n",
    "        ORDER BY last_activity ASC\n",
    "    \"\"\")\n",
    "    return [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "list_sessions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we see that the user has created two sessions. But the problem here is that user won't be able to view the context of the session as the list displayed is just the session IDs.\n",
    "\n",
    "To allow the user to view the context of the session, we created a feature that automatically generates a title for the session based on the last message. You may view the whole orchestration of the session title generation in `app/services/chat_sessions.py`.\n",
    "\n",
    "For now, we will discuss the functions on a function-by-function basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For generating the session title, we used the following chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Introduction to AI Chatbots\"\n"
     ]
    }
   ],
   "source": [
    "# NOTE: The variables in this notebook are based on the active session I have in my local machine.\n",
    "# You may need to change the values if you want to run the code.\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_ollama import ChatOllama\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OLLAMA_MODEL = os.getenv(\"OLLAMA_MODEL\", \"llama3.2\")\n",
    "OLLAMA_URL = os.getenv(\"OLLAMA_URL\", \"http://localhost:11434\")\n",
    "\n",
    "def get_session_title_prompt():\n",
    "    return ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful assistant that creates concise, descriptive titles for chat sessions. Create a title that captures the main topic or purpose of the conversation. \\\n",
    "        Create a short title (3-5 words) for a chat session that starts with the message given. Do not include any other text in your response. \\\n",
    "        If the message is vague -- no context or just a greeting, return 'New Chat'\"),\n",
    "        (\"user\", \"{message}\")\n",
    "    ])\n",
    "\n",
    "# give it a try\n",
    "llm = ChatOllama(model=OLLAMA_MODEL, base_url=OLLAMA_URL)\n",
    "\n",
    "get_title_chain = get_session_title_prompt() | llm\n",
    "\n",
    "response = get_title_chain.invoke({\"message\": \"Describe what is an AI chatbot for beginners!\"})\n",
    "\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I have added an additional instruction to not generate a title if the message is vague:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Chat\n"
     ]
    }
   ],
   "source": [
    "vague_message = \"Hello, how are you?\"\n",
    "\n",
    "response = get_title_chain.invoke({\"message\": vague_message})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to avoid the LLM from generating a title that is too vague, thus retaining the title as \"New Chat\". When the user has provided enough context already, an additional function was written so that the session is renamed using up to the last 4 questions and answers, to be discussed in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generating the session title, it is stored in the database along with the session ID to make sure that the title matches the correct session. See below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session ID: c2b34f93-1dfa-49a0-9bc3-899c3465314e, Title: \"Introduction to AI Chatbots Basics\"\n",
      "Session ID: 9029ee99-c701-468a-820d-addb23ff1281, Title: The Meaning of Existence and Purpose\n"
     ]
    }
   ],
   "source": [
    "# NOTE: The variables in this notebook are based on the active session I have in my local machine.\n",
    "# You may need to change the values if you want to run the code.\n",
    "\n",
    "SESSION_TITLES_TABLE = os.getenv(\"SESSION_TITLES_TABLE\", \"session_titles\")\n",
    "\n",
    "cursor = get_connection().cursor()\n",
    "cursor.execute(f\"SELECT session_id, title FROM {SESSION_TITLES_TABLE}\")\n",
    "\n",
    "result = cursor.fetchall()\n",
    "\n",
    "for row in result:\n",
    "    print(f\"Session ID: {row[0]}, Title: {row[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have the session titles in the database!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Frontend: Streamlit Chat UI\n",
    "\n",
    "Streamlit provides a fast and intuitive way to build interactive UIs using pure Python. One of its biggest advantages is that it allows **direct integration with backend functions**, eliminating the need to build endpoints. This enables faster iteration and keeps the project focused on core functionality rather than API boilerplate.\n",
    "\n",
    "By using Streamlit, we were able to rapidly prototype and test the chatbot interface while maintaining tight coupling with LangChain and the chat history logic.\n",
    "\n",
    "**Key parts of the UI:**\n",
    "- Session selection sidebar\n",
    "- User input form using `st.chat_input()`\n",
    "- Real-time response display using `st.write_stream()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We divided the frontend into two components: the sidebar and the main chat interface.\n",
    "\n",
    "The sidebar is used to select the session and create a new session. It also displays the chat history.\n",
    "\n",
    "The main chat interface is used to display the chat history and the user input form. It also displays the response from the LLM.\n",
    "\n",
    "A screenshot of the frontend is shown below:\n",
    "\n",
    "![chat-history](public/images/chat-history.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the streamlit code for the sidebar:\n",
    "\n",
    "```python\n",
    "# session management\n",
    "if \"sessions\" not in st.session_state:\n",
    "    st.session_state[\"sessions\"] = [str(session_id) for session_id in list_sessions()]\n",
    "\n",
    "# default session_id is None\n",
    "if \"session_id\" not in st.session_state:\n",
    "    st.session_state[\"session_id\"] = None\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"Ollama Chatbot\",\n",
    ")\n",
    "\n",
    "with st.sidebar:\n",
    "        \n",
    "    # create new session button\n",
    "    st.button(\"New Chat\", on_click=create_new_session, use_container_width=True)\n",
    "    \n",
    "    st.divider()\n",
    "    \n",
    "    st.header(\"Chat History\")\n",
    "\n",
    "    # list session history\n",
    "    for session in st.session_state[\"sessions\"]:\n",
    "        session_title_col, delete_col = st.columns([4, 1])\n",
    "        with session_title_col:\n",
    "            session_title = get_session_title_sync(session) or f\"New Chat\" # get session title or default to \"New Chat\" if no title yet\n",
    "\n",
    "            if session == st.session_state[\"session_id\"]:\n",
    "                st.button(\n",
    "                    session_title,\n",
    "                    key=f\"session_{session}\",\n",
    "                    use_container_width=True,\n",
    "                    type=\"primary\" # emphasize the current session\n",
    "                )\n",
    "            else:\n",
    "                if st.button(\n",
    "                    session_title,\n",
    "                    key=f\"session_{session}\",\n",
    "                    use_container_width=True\n",
    "                ):\n",
    "                    st.session_state[\"session_id\"] = session \n",
    "                    st.session_state[\"history\"] = []\n",
    "                    st.rerun()\n",
    "        with delete_col: # delete session button\n",
    "            if st.button(\"üóëÔ∏è\", key=f\"delete_{session}\", use_container_width=True):\n",
    "                st.session_state[\"sessions\"].remove(session)\n",
    "                run_async(delete_session(session))\n",
    "                st.rerun()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the user wants a new session, they can click the \"New Chat\" button in the sidebar. This will create a new page with no session ID (see below):\n",
    "\n",
    "![newchat](public/images/new-chat.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The session ID will not be generated until the user enters the first message. This is to avoid generating a session ID every time the user clicks the \"New Chat\" button."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, when a user enters a query, the following logic is executed:\n",
    "\n",
    "1. If the session ID is not set, a new session is created and the session ID is set.\n",
    "2. If the session ID is set, the query is sent to the LLM.\n",
    "3. The response from the LLM is displayed in the chat interface.\n",
    "4. The chat history is updated with the new message and the response from the LLM.\n",
    "5. The session title is updated asynchronously with the new message.\n",
    "6. The chat history is displayed in the chat interface.\n",
    "7. The session title is displayed in the sidebar.\n",
    "\n",
    "A sample code is shown below:\n",
    "\n",
    "```python\n",
    "# user input\n",
    "user_input = st.chat_input(\"Type your message here...\", key=\"user_input\")\n",
    "if user_input and user_input != \"\":\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(f\"{user_input}\")\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        if session_id is None:  # if this is a new chat, create a new session\n",
    "            session_id = str(uuid.uuid4())\n",
    "            st.session_state[\"session_id\"] = session_id\n",
    "            st.session_state[\"sessions\"] = [session_id] + st.session_state[\"sessions\"] # add new session to the top of the list\n",
    "            thread_pool.submit(run_async, create_session_title(session_id, user_input)) # create session title asynchronously\n",
    "            st.write_stream(get_response_stream(session_id, user_input))\n",
    "            st.rerun()\n",
    "        else:\n",
    "            st.write_stream(get_response_stream(session_id, user_input))\n",
    "            if get_session_title_sync(session_id) == \"New Chat\": # rename session after getting more information from the session\n",
    "                message_history = get_chat_history(session_id)\n",
    "                messages = \"\\n\".join(f\"{msg['type']}: {msg['content']}\" for msg in message_history[-4:]) \n",
    "                new_title = run_async(update_session_title(session_id, messages))\n",
    "                if new_title:\n",
    "                    st.session_state[\"session_title\"] = new_title\n",
    "            st.rerun()  # refresh the UI to update sidebar and title\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the previous section, when the first message is vague, the session title is set to \"New Chat\". A logic was added in the streamlit app to check if the session title is \"New Chat\" and if so, update the session title with the last 4 messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, a streaming response `st.write_stream()` was used to display the response from the LLM in real-time. This will allow the user to see the response as it is being generated, rather than waiting for the entire response to be generated before displaying it, allowing for a more natural conversation experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Running the Chatbot\n",
    "\n",
    "We have created a containerized application that can be run locally.\n",
    "\n",
    "To run the application, you need to have the following:\n",
    "\n",
    "- Docker\n",
    "- Docker Compose\n",
    "\n",
    "To run the application, run the following command:\n",
    "\n",
    "```bash\n",
    "docker compose up --build\n",
    "```\n",
    "\n",
    "After the build is complete, you can access the application at `http://localhost:8501`.\n",
    "\n",
    "For more information on how to run the application, please refer to the [README](README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Tests\n",
    "\n",
    "We wrote unit tests for:\n",
    "- Chat streaming logic\n",
    "- Chat history retrieval\n",
    "- Database session listing\n",
    "\n",
    "For the integration tests, we used `pytest` with mocking to isolate components.\n",
    "\n",
    "Run the tests with the following command when you are running via docker compose:\n",
    "\n",
    "```bash\n",
    "docker-compose exec app pytest\n",
    "```\n",
    "\n",
    "If running locally, you can run the tests with the following command:\n",
    "\n",
    "```bash\n",
    "pytest\n",
    "```\n",
    "\n",
    "View [README](README.md) for more information on how to run the tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Challenges faced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Execution of Title Creation and Response Generation\n",
    "\n",
    "One of the key challenges I encountered during development was managing the execution timing of two separate backend tasks:\n",
    "- Generating the session title\n",
    "- Streaming the chatbot response\n",
    "\n",
    "Initially, these were executed sequentially. This led to noticeable delays and a less responsive UI. To solve this, I introduced `asyncio` to run both tasks in parallel, since they are independent of each other. This reduced the overall wait time and significantly improved the user experience. This idea has also led me to update other operations, like delete a session, to be asynchronous as well to further reduce the frontend latency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Containerizing the Application\n",
    "\n",
    "Another challenge was containerizing the whole application (App, Postgres, and Ollama). While I had limited experience with Docker previously, I decided to adopt it for this project to make the app:\n",
    "- More portable\n",
    "- Easier to run \n",
    "\n",
    "Through this, I gained more experience with Dockerfile, docker-compose, and handling service-to-service networking (e.g., database and model container communication). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This project demonstrates the design and development of a local, self-contained LLM chatbot using modern tooling:\n",
    "- **Ollama** for efficient local LLM inference\n",
    "- **LangChain** for chaining model logic and integrating memory\n",
    "- **PostgreSQL** for persistent session memory\n",
    "- **Streamlit** for an interactive and minimal frontend\n",
    "- **Docker** for full application portability and environment consistency\n",
    "\n",
    "Throughout the process, I focused on modularity, performance, and user experience. Asynchronous logic was introduced to parallelize tasks like response generation and session title creation. Containerization was adopted to improve deployability and reproducibility, even though it initially posed a learning curve."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
